{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## Niyaz Gubaidullin",
   "id": "ef5444bdb5bbebfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task 1: Exploratory Data Analysis (EDA) and Preprocessing (25 points)",
   "id": "92dbb9f7559bb30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load the Data\n",
    "- **Read `opsd_raw.csv` into a DataFrame.** Identify the relevant columns.\n",
    "- **Manually confirm** which columns are for Denmark’s power load and productions by referencing `README.md`."
   ],
   "id": "4f32be2feef102ac"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.read_csv('opsd_raw.csv')\n",
    "required_columns = [\n",
    "    'utc_timestamp',\n",
    "    'DK_load_actual_entsoe_transparency',\n",
    "    'DK_wind_generation_actual',\n",
    "    'DK_solar_generation_actual'\n",
    "]\n",
    "df = df[required_columns]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Data Inspection\n",
    "- Print the shape (rows × columns) of the raw data and the first few lines.\n",
    "- Check for missing values (NaNs). Decide how you’ll handle them (drop rows,\n",
    "fill forward, etc.)."
   ],
   "id": "91847e901233d7f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"=== Data Inspection ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "df = df.dropna()"
   ],
   "id": "74420964b0e12925",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that we have only **15 nil values** over **50,000 lines**. We can just remove them.\n",
    "\n",
    "### Missing values:\n",
    "- `utc_timestamp`: **0** missing values\n",
    "- `DK_load_actual_entsoe_transparency`: **2** missing values\n",
    "- `DK_wind_generation_actual`: **2** missing values\n",
    "- `DK_solar_generation_actual`: **11** missing values\n"
   ],
   "id": "f174d1e65be33e14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Form 24-Hour Arrays and Label Seasons.\n",
    "- Convert the hourly data into daily slices of length 24.\n",
    "- Ensure each daily record lines up with a single date (e.g., from midnight to\n",
    "midnight).\n",
    "- Show at least 5 sample arrays to confirm correctness."
   ],
   "id": "45bddba2f7f98fd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "##function to get season by date\n",
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if 3 <= month <= 5:\n",
    "        return 'spring'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'summer'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'autumn'\n",
    "    else:\n",
    "        return 'winter'\n",
    "\n",
    "\n",
    "df['utc_timestamp'] = pd.to_datetime(df['utc_timestamp'])\n",
    "df = df.sort_values('utc_timestamp')\n",
    "df['date'] = df['utc_timestamp'].dt.date\n",
    "df['season'] = df['utc_timestamp'].apply(get_season)\n",
    "\n",
    "daily_data = []\n",
    "for date, group in df.groupby('date'):\n",
    "    if len(group) == 24:  # we only add full days\n",
    "        daily_data.append({\n",
    "            'date': date,\n",
    "            'season': group['season'].iloc[0],\n",
    "            'load_24h': group['DK_load_actual_entsoe_transparency'].values,\n",
    "            'wind_24h': group['DK_wind_generation_actual'].values,\n",
    "            'solar_24h': group['DK_solar_generation_actual'].values\n",
    "        })\n",
    "\n",
    "daily_df = pd.DataFrame(daily_data)\n",
    "print(\"\\n=== 5 Sample Days ===\")\n",
    "for i in range(5):  ##print first 5 days\n",
    "    print(f\"\\nDate: {daily_df['date'].iloc[i]}, Season: {daily_df['season'].iloc[i]}\")\n",
    "    print(f\"Load (first 5h): {daily_df['load_24h'].iloc[i]}\")\n",
    "    print(f\"Wind (first 5h): {daily_df['wind_24h'].iloc[i]}\")\n",
    "    print(f\"Solar (first 5h): {daily_df['solar_24h'].iloc[i]}\")\n",
    "\n",
    "print(\"\\n=== Season Distribution ===\")\n",
    "print(daily_df['season'].value_counts())\n",
    "\n",
    "##seson distribution:\n",
    "# === Season Distribution ===\n",
    "# season\n",
    "# spring    552\n",
    "# summer    552\n",
    "# winter    510\n",
    "# autumn    484\n",
    "# Name: count, dtype: int64\n",
    "\n"
   ],
   "id": "1da1e264258b9aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Train-test split.\n",
    "- Split the daily records into train (70%), validation (15%), and test\n",
    "(15%) sets."
   ],
   "id": "78da9e3fdcbabcf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df, temp_df = train_test_split(daily_df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ],
   "id": "adf13579d2605ebd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Data scaling.\n",
    "- Apply one of the standardisation (or scaling) methods you've learnt on\n",
    "the IML course on every feature column. Avoid data leakage when fit a scaler."
   ],
   "id": "6d863d758064ca4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def scale_data(df, scalers, fit=False):\n",
    "    scaled = df.copy()\n",
    "    for feature, scaler in scalers.items():\n",
    "        data = np.vstack(scaled[f'{feature}_24h'])\n",
    "\n",
    "        if fit:\n",
    "            ## we train scaler with train data\n",
    "            scaled_data = scaler.fit_transform(data)\n",
    "        else:\n",
    "            ## here we just scale data, no train\n",
    "            scaled_data = scaler.transform(data)\n",
    "        scaled[f'{feature}_24h'] = [row.tolist() for row in scaled_data]\n",
    "    return scaled\n",
    "\n",
    "\n",
    "features = ['load_24h', 'wind_24h', 'solar_24h']\n",
    "scaler = StandardScaler()\n",
    "scalers = {\n",
    "    'load': StandardScaler(),\n",
    "    'wind': StandardScaler(),\n",
    "    'solar': StandardScaler()\n",
    "}\n",
    "train_df_scaled = scale_data(train_df, scalers, fit=True)\n",
    "val_df_scaled = scale_data(val_df, scalers)\n",
    "test_df_scaled = scale_data(test_df, scalers)\n",
    "\n",
    "## here we check results of scaling. Mean nust be 0 while std must be 1\n",
    "for feature in ['load', 'wind', 'solar']:\n",
    "    data = np.vstack(train_df_scaled[f'{feature}_24h'])\n",
    "    print(f\"{feature} - mean: {data.mean():.2f}, std: {data.std():.2f}\")"
   ],
   "id": "20edf0ea8b79907c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Brief Analysis\n",
    "- Plot at least one example of a daily consumption profile for each season.\n",
    "- Include 1–2 personal observations (e.g., “We see that winter consumption is\n",
    "generally higher than summer.”). This helps ensure you’re truly analyzing the\n",
    "data beyond a purely automated approach."
   ],
   "id": "cd7f4e184606c584"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I will plot first day of each season after scaling data",
   "id": "6db5f07eeb0e2d03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(18, 12))\n",
    "seasons = ['winter', 'spring', 'summer', 'autumn']\n",
    "\n",
    "for i, season in enumerate(seasons, 1):\n",
    "    # take first day of each season\n",
    "    season_data = train_df_scaled[train_df_scaled['season'] == season].iloc[0]\n",
    "\n",
    "    plt.subplot(2, 2, i)\n",
    "\n",
    "    # plot lines for load, solar and wind\n",
    "    plt.plot(season_data['load_24h'], label='Load', color='blue')\n",
    "    plt.plot(season_data['wind_24h'], label='Wind', color='green')\n",
    "    plt.plot(season_data['solar_24h'], label='Solar', color='orange')\n",
    "\n",
    "    plt.title(f'{season.capitalize()} (Scaled)')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Scaled Value')\n",
    "    plt.axhline(y=0, color='gray', linestyle='--')  # Нулевая линия\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c6ccdd5a26c649e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- We may notice that energy consumption is much higher in winter than in summer.\n",
    "- There is also more solar energy generated in summer than in winter."
   ],
   "id": "76ed0cafe3230414"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 2: Baseline MLP (Fully-Connected Network) (15 points)",
   "id": "daaf77d78e1243b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Implement an MLP in PyTorch with at least:\n",
    "- One hidden layer (e.g., 32–128 neurons).\n",
    "- Non-linear activation (e.g., ReLU)."
   ],
   "id": "11e3965c5ac07458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    #These parameters provided an optimal balance between the learning rate and the quality of the model for this task. hidden_size=64 and Learning Rate (0.001):\n",
    "    def __init__(self, input_size=72, hidden_size=64, output_size=4):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "6bf41175eda07bab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Input Representation\n",
    "- Flatten the 24-hour sequence into a 24-dimensional vector.\n",
    "- Normalize or standardize the inputs if needed."
   ],
   "id": "e1548479586a0b3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_data(df):\n",
    "    features = np.array([np.concatenate([day['load_24h'],\n",
    "                                         day['wind_24h'],\n",
    "                                         day['solar_24h']])\n",
    "                         for day in df.to_dict('records')])\n",
    "    season_to_idx = {'winter': 0, 'spring': 1, 'summer': 2, 'autumn': 3}\n",
    "    labels = np.array([season_to_idx[day['season']] for day in df.to_dict('records')])\n",
    "    return torch.FloatTensor(features), torch.LongTensor(labels)"
   ],
   "id": "c7055c2e330249e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Train & Evaluate\n",
    "- Show training curves for loss and accuracy.\n",
    "- Evaluate on both validation and test sets, and report final accuracy."
   ],
   "id": "8a50e5711f5ce3a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## here we train model with 50 epochs and 32 batch_size\n",
    "\n",
    "def train_model(train_df, val_df, epochs=50, batch_size=32):\n",
    "    # prepare data\n",
    "    X_train, y_train = prepare_data(train_df)\n",
    "    X_val, y_val = prepare_data(val_df)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # create model\n",
    "    model = MLP(input_size=72, hidden_size=64, output_size=4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    #save data for plot\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc = accuracy_score(y_val.numpy(), val_predicted.numpy())\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss.item())\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # print results of epochs\n",
    "        print(f'Epoch {epoch + 1}/{epochs}: '\n",
    "              f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss.item():.4f}, '\n",
    "              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "\n",
    "model, train_losses, val_losses, train_accs, val_accs = train_model(train_df_scaled, val_df_scaled, epochs=50)"
   ],
   "id": "2cf37397f76001a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check accurancy on test data, print accuracy, final “test set” predictions and classification report",
   "id": "3d8a09d00739590f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- we have test accuracy 0.8730\n",
    "- Classification Report: to view it run code"
   ],
   "id": "951e56cabc00b9ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, test_df):\n",
    "    X_test, y_test = prepare_data(test_df)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = accuracy_score(y_test.numpy(), predicted.numpy())\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    print(\"\\nTest Set Predictions:\")\n",
    "    print(predicted.numpy())\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test.numpy(), predicted.numpy(),\n",
    "                                target_names=['winter', 'spring', 'summer', 'autumn']))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "evaluate_model(model, test_df_scaled)"
   ],
   "id": "d1fc473d94217058",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plots",
   "id": "adf1fd608018b4be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_metrics(train_losses, val_losses, train_accs, val_accs):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, train_accs, val_accs)"
   ],
   "id": "451788596302b5dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 3: 1D-CNN on Raw Time-Series (20 points)",
   "id": "272d25c45c50896f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. 1D Convolution Architecture\n",
    "- Use PyTorch Conv1d layers to process 3-channels sequences of length 24.\n",
    "- At least one convolutional layer, one pooling layer, and a final fully connected\n",
    "layer for classification."
   ],
   "id": "cf2335f9e8115f8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_channels=3, seq_len=24, num_classes=4):\n",
    "        super(CNN1D, self).__init__()\n",
    "\n",
    "        # (batch_size, 3, 24) -> (batch_size, 32, 22)\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=3, padding='valid')\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # (batch_size, 32, 22) -> (batch_size, 32, 11)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # (batch_size, 32 * 11) -> (batch_size, 64)\n",
    "        self.fc1 = nn.Linear(32 * 11, 64)\n",
    "\n",
    "        # (batch_size, 64) -> (batch_size, 4)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def prepare_cnn_data(df):\n",
    "    features = np.array([np.stack([day['load_24h'],\n",
    "                                   day['wind_24h'],\n",
    "                                   day['solar_24h']], axis=0)\n",
    "                         for day in df.to_dict('records')])\n",
    "\n",
    "    season_to_idx = {'winter': 0, 'spring': 1, 'summer': 2, 'autumn': 3}\n",
    "    labels = np.array([season_to_idx[day['season']] for day in df.to_dict('records')])\n",
    "\n",
    "    return torch.FloatTensor(features), torch.LongTensor(labels)"
   ],
   "id": "ec85cf01923e7423",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train and validate cnn",
   "id": "529758cb3c863e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_cnn(train_df, val_df, epochs=50, batch_size=32):\n",
    "    X_train, y_train = prepare_cnn_data(train_df)\n",
    "    X_val, y_val = prepare_cnn_data(val_df)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = CNN1D(input_channels=3, seq_len=24, num_classes=4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc = accuracy_score(y_val.numpy(), val_predicted.numpy())\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss.item())\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}: '\n",
    "              f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss.item():.4f}, '\n",
    "              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "model_cnn, train_losses_cnn, val_losses_cnn, train_accs_cnn, val_accs_cnn = train_cnn(\n",
    "        train_df_scaled, val_df_scaled, epochs=50)\n"
   ],
   "id": "fe90038d35018a18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CNN Test Accurancy, Test Set Predictions and Classification Report",
   "id": "674609275af14a7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_cnn(model, test_df):\n",
    "    X_test, y_test = prepare_cnn_data(test_df)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        accuracy = accuracy_score(y_test.numpy(), predicted.numpy())\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "        print(\"\\nTest Set Predictions:\")\n",
    "        print(predicted.numpy())\n",
    "\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test.numpy(), predicted.numpy(),\n",
    "                                    target_names=['winter', 'spring', 'summer', 'autumn']))\n",
    "\n",
    "    return\n",
    "\n",
    "test_acc = evaluate_cnn(model_cnn, test_df_scaled)\n",
    "\n"
   ],
   "id": "b9b0ad36ed295c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MLP vs CNN\n",
    "\n",
    "- CNN accuracy: 0.8889\n",
    "- MLP accuracy: 0.8730"
   ],
   "id": "2330bde08f0cdc21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 4: 2D Transform & 2D-CNN (20 points)",
   "id": "3a9749582ef7f3f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Choose a transformation from the possible options:\n",
    "- Option A: Use pyts (e.g., GramianAngularField, RecurrencePlot, or\n",
    "MarkovTransitionField).\n",
    "- Option B: Create a simple matrix that arranges the data in a 2D pattern\n",
    "(though 24 points is small, you might artificially reshape it into 6×4 or some\n",
    "other arrangement).\n",
    "- Option C: Another custom transformation that you can justify"
   ],
   "id": "334a5e6ff82196eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " #### GAF is the best choice, as it transforms time series into a spatial representation that preserves seasonal patterns, which is ideal for 2D CNNs. This is also confirmed by practice: GAF is often used in time series classification tasks through images (for example, in energy and bioinformatics).",
   "id": "9864f9cb7c849864"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pyts.image import GramianAngularField\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "def transform_to_gaf(data_2d, image_size=24):\n",
    "    transformer = GramianAngularField(image_size=image_size, method='summation')\n",
    "    return transformer.transform(data_2d)\n",
    "\n",
    "\n",
    "def apply_gaf_transformation(df):\n",
    "    df_gaf = df.copy()\n",
    "    features = ['load', 'wind', 'solar']\n",
    "\n",
    "    for feature in features:\n",
    "        data = np.vstack(df[f'{feature}_24h'])\n",
    "        gaf_images = transform_to_gaf(data)\n",
    "        df_gaf[f'{feature}_gaf'] = [img for img in gaf_images]\n",
    "\n",
    "    return df_gaf\n",
    "\n",
    "\n",
    "# Dataset и DataLoader\n",
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, df, labels):\n",
    "        self.load_images = np.stack(df['load_gaf'])\n",
    "        self.wind_images = np.stack(df['wind_gaf'])\n",
    "        self.solar_images = np.stack(df['solar_gaf'])\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.stack([\n",
    "            self.load_images[idx],\n",
    "            self.wind_images[idx],\n",
    "            self.solar_images[idx]\n",
    "        ], axis=0)\n",
    "        return torch.FloatTensor(image), torch.LongTensor([self.labels[idx]])"
   ],
   "id": "caf643e9dceb0de5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Train a 2D CNN\n",
    "- Use standard 2D convolutions (Conv2d in PyTorch).\n",
    "- At least one pooling layer, at least one hidden conv layer, and a final fullyconnected block."
   ],
   "id": "91914f7a54bb3519"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SeasonClassifier2DCNN(nn.Module): #CNN2D class\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(SeasonClassifier2DCNN, self).__init__()\n",
    "        # Input shape: (batch_size, 3, 24, 24)\n",
    "        # Output shape: (batch_size, 16, 24, 24) [padding=1 preserves spatial dims]\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()  # Shape preserved: (batch_size, 16, 24, 24)\n",
    "\n",
    "        # Input: (batch_size, 16, 24, 24)\n",
    "        # Output: (batch_size, 16, 12, 12) [kernel_size=2 halves the dimensions]\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Input: (batch_size, 16, 12, 12)\n",
    "        # Output: (batch_size, 32, 12, 12) [padding=1 preserves spatial dims]\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()  # Shape preserved: (batch_size, 32, 12, 12)\n",
    "\n",
    "        # Input: (batch_size, 32, 12, 12)\n",
    "        # Output: (batch_size, 32, 6, 6) [kernel_size=2 halves the dimensions]\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Flatten occurs here in forward(): (batch_size, 32, 6, 6) -> (batch_size, 32*6*6=1152)\n",
    "\n",
    "        # Input: (batch_size, 1152)\n",
    "        # Output: (batch_size, 128)\n",
    "        self.fc1 = nn.Linear(32 * 6 * 6, 128)\n",
    "        self.relu3 = nn.ReLU()  # Shape preserved: (batch_size, 128)\n",
    "\n",
    "        # Input: (batch_size, 128)\n",
    "        # Output: (batch_size, num_classes)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=20): ## train model\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels.squeeze()).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels.squeeze()).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        #dispaly results\n",
    "        print(f'Epoch {epoch + 1}/{epochs}: '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n"
   ],
   "id": "d7778981f7e7ea83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train model, print train, validation accuracy",
   "id": "99109992f49c07e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_gaf = apply_gaf_transformation(train_df_scaled)\n",
    "val_gaf = apply_gaf_transformation(val_df_scaled)\n",
    "test_gaf = apply_gaf_transformation(test_df_scaled)\n",
    "\n",
    "# 3. Подготовка DataLoader\n",
    "le = LabelEncoder()\n",
    "train_labels = le.fit_transform(train_gaf['season'])\n",
    "val_labels = le.transform(val_gaf['season'])\n",
    "test_labels = le.transform(test_gaf['season'])\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = EnergyDataset(train_gaf, train_labels)\n",
    "val_dataset = EnergyDataset(val_gaf, val_labels)\n",
    "test_dataset = EnergyDataset(test_gaf, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# 4. Создание и обучение модели\n",
    "model = SeasonClassifier2DCNN()\n",
    "train_losses, val_losses, val_accuracies, d = train_model(model, train_loader, val_loader)"
   ],
   "id": "e0b70930934a58ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test model on test data, classification report",
   "id": "2ea74b0f590255f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.squeeze()).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.squeeze().cpu().numpy())\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Classification Report\n",
    "class_names = ['winter', 'spring', 'summer', 'autumn']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=class_names,\n",
    "    digits=4\n",
    "))"
   ],
   "id": "3e703c1d7cfb1908",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Comparison Results\n",
    "\n",
    "**1D-CNN** showed better accuracy (**0.8889**) than:\n",
    "- MLP (**0.8730**)\n",
    "- 2D-CNN (**0.8190**)\n",
    "\n",
    "## Key Observations:\n",
    "- The **GAF conversion**, although visually interpretable, **did not improve accuracy**\n",
    "  → Likely due to the loss of temporal detail during 2D transformation.\n",
    "\n",
    "## Conclusion:\n",
    "For this specific task, **raw time series (1D-CNN)** proved more efficient than image-based (2D-CNN) approaches."
   ],
   "id": "77afc5df04cc391"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explanation of Gramian Angular Field (GAF) Choice\n",
    "\n",
    "The **Gramian Angular Field (GAF)** was chosen for transforming 1D time-series data into 2D images because it effectively preserves temporal correlations by encoding values into polar coordinates and calculating trigonometric relationships. This method is particularly useful for periodic or seasonal patterns (like energy consumption data), as it highlights:\n",
    "\n",
    "- **Time-dependent dynamics**: Each point in the GAF matrix represents the temporal interaction between two time steps.\n",
    "\n",
    "- **Invariance to scaling**: The angular representation normalizes amplitude variations, focusing on shape.\n",
    "\n",
    "## Why GAF helps in this task:\n",
    "\n",
    "1. **Seasonal Patterns**: Solar generation has strong daily/seasonal cycles, which GAF captures as distinct diagonal structures.\n",
    "\n",
    "2. **Multi-channel compatibility**: GAF processes each feature (load/wind/solar) separately, allowing the CNN to learn cross-feature spatial patterns.\n",
    "\n",
    "> **Citation**:\n",
    "> *\"The Gramian Angular Field (GAF) represents time series in a polar coordinate system, followed by a trigonometric operation to identify temporal correlations\"*\n",
    "> — PyTS Documentation, https://pyts.readthedocs.io/en/stable/generated/pyts.image.GramianAngularField.html"
   ],
   "id": "140278c6ec8c048e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
